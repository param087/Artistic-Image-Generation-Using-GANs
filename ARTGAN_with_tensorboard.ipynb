{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artistic Image Generation Using Generative Adverserial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Next --- shells are used only if you are using Google Colab platform to connect your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jxtiYQtl_hsW",
    "outputId": "c98f6250-490d-48d1-c270-b308f48a6a75"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('drive/seminar') #locate your folder in drive\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative way to connect to your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/seminar') #locate your folder in drive this is main\n",
    "                                            #difference in the method to mount the drive\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Install Dependacies required for Program on Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ujZVDoRR_tRM",
    "outputId": "13d3ce30-c2e8-4f2c-f800-8f199d8d7299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "rV7b9FJ_AWpA",
    "outputId": "dddcfbac-262e-439b-cc3e-3f3998cff95f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTGAN.ipynb  LandGAN.ipynb  newLand2  __pycache__\t   train_64\r\n",
      "data\t      model\t     newLand3  seminar report.odt  train_landscape_128\r\n",
      "drive\t      model2\t     newLand4  tensorboard\t   utils.py\r\n",
      "GANART.ipynb  newLand\t     newLand5  train_128\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# IF your are running in your on computer the follow from here\n",
    "\n",
    " ***\n",
    "### *Note* - please chech the current folder using os library and folder name. If folder is not present in your computer create on in current directory\n",
    "### For Tensorboard uncomment the code related tensorboard\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1938
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "a7XA5aLYAkqv",
    "outputId": "54f937ef-530d-4ba6-cc31-5bd7131bd512"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import datetime\n",
    "\n",
    "HEIGHT, WIDTH, CHANNEL = 128, 128, 3\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 5000\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "version = \"newLand4\" #-----newLand4\n",
    "newLand_path = version\n",
    "\n",
    "def lrelu(x, n, leak=0.2):\n",
    "  return tf.maximum(x, leak * x, name=n)\n",
    "\n",
    "\n",
    "def process_data():\n",
    "    current_dir = os.getcwd()\n",
    "    landscape_dir = os.path.join(current_dir, 'train_landscape_128')\n",
    "    images = []\n",
    "    for each in os.listdir(landscape_dir):\n",
    "        images.append(os.path.join(landscape_dir, each))\n",
    "\n",
    "    all_images = tf.convert_to_tensor(images, dtype=tf.string)\n",
    "\n",
    "    images_queue = tf.train.slice_input_producer([all_images])\n",
    "    \n",
    "    content = tf.read_file(images_queue[0])\n",
    "    image = tf.image.decode_jpeg(content, channels=CHANNEL)\n",
    "\n",
    "    #--temp-----------\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    \n",
    "    size = [HEIGHT, WIDTH]\n",
    "    \n",
    "    image = tf.image.resize_images(image, size)\n",
    "    image.set_shape([HEIGHT, WIDTH, CHANNEL])\n",
    "    \n",
    "\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.0\n",
    "    \n",
    "\n",
    "    iamges_batch = tf.train.shuffle_batch([image],\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         num_threads = 4,\n",
    "                                         capacity = 2000 + 3*BATCH_SIZE,\n",
    "                                         min_after_dequeue = 2000)\n",
    "    \n",
    "    num_images = len(images)\n",
    "    return iamges_batch, num_images\n",
    "    \n",
    "\n",
    "\n",
    "def generator(input, random_dim, is_train, reuse=False):\n",
    "\n",
    "    c4, c8, c16, c32, c64 = 512, 256, 128, 64, 32\n",
    "    s4 = 4\n",
    "    \n",
    "    output_dim = CHANNEL\n",
    "    with tf.variable_scope('gen') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        w1 = tf.get_variable('w1', shape=[random_dim, s4*s4*c4],dtype=tf.float32,initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable('b1', shape=[c4*s4*s4], dtype=tf.float32,initializer = tf.constant_initializer(0.0))\n",
    "        flat_conv1 = tf.add( tf.matmul(input, w1), b1,name=\"flat_conv1\")\n",
    "\n",
    "    \n",
    "        #4x4x512\n",
    "        conv1 = tf.reshape(flat_conv1, shape=[-1, s4, s4, c4], name='conv1')\n",
    "        bn1 = tf.contrib.layers.batch_norm(conv1, is_training=is_train,epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn1')\n",
    "        act1 = tf.nn.relu(bn1, name='act1')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"g_act1\", act1)\n",
    "        #------------\n",
    "\n",
    "        #8x8x256\n",
    "        conv2 = tf.layers.conv2d_transpose(act1, c8, kernel_size=[5,5], strides=[2,2], padding=\"SAME\",\n",
    "                                          kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), name='conv2')\n",
    "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn2')\n",
    "        act2 = tf.nn.relu(bn2, name='act2')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"g_act2\", act2)\n",
    "        #------------\n",
    "        \n",
    "\n",
    "        #16x16x128\n",
    "        conv3 = tf.layers.conv2d_transpose(act2, c16, kernel_size=[5,5], strides=[2,2], padding=\"SAME\",\n",
    "                                          kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), name='conv3')\n",
    "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn3')\n",
    "        act3 = tf.nn.relu(bn3, name='act3')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"g_act3\", act3)\n",
    "        #------------\n",
    "        \n",
    "        #32x32x64\n",
    "        conv4 = tf.layers.conv2d_transpose(act3, c32, kernel_size=[5,5], strides=[2,2], padding=\"SAME\",\n",
    "                                          kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), name='conv4')\n",
    "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn4')\n",
    "        act4 = tf.nn.relu(bn4, name='act4')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"g_act4\", act4)\n",
    "        #------------\n",
    "\n",
    "        #64x64x32\n",
    "        conv5 = tf.layers.conv2d_transpose(act4, c64, kernel_size=[5,5], strides=[2,2], padding=\"SAME\",\n",
    "                                          kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), name='conv5')\n",
    "        bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn5')\n",
    "        act5 = tf.nn.relu(bn5, name='act5')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"g_act5\", act5)\n",
    "        #------------\n",
    "        \n",
    "\n",
    "        #128x128x3\n",
    "        conv6 = tf.layers.conv2d_transpose(act5, output_dim, kernel_size=[5,5], strides=[2,2], padding=\"SAME\",\n",
    "                                          kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), name='conv6')\n",
    "        #bn6 = tf.contrib.layers.batch_norm(conv6, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, name='bn6')\n",
    "        act6 = tf.nn.tanh(conv6, name='act6')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"g_act6\", act6)\n",
    "        #------------\n",
    "        \n",
    "        return act6\n",
    "\n",
    "    \n",
    "def discriminator(input, is_train, reuse=False):\n",
    "    c2, c4, c8, c16 = 64, 128, 256, 512\n",
    "    with tf.variable_scope('dis') as scope:\n",
    "\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        #64x64x64\n",
    "        conv1 = tf.layers.conv2d(input, c2, kernel_size=[5,5], strides=[2, 2], padding=\"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv1')\n",
    "        #bn1 = tf.contrib.layers.batch_norm(conv1, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn1')\n",
    "        act1 = lrelu(conv1, n='act1')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"d_act1\", act1)\n",
    "        #------------\n",
    "\n",
    "        #32x32x128\n",
    "        conv2 = tf.layers.conv2d(act1, c4, kernel_size=[5,5], strides=[2, 2], padding=\"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv2')\n",
    "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn2')\n",
    "        act2 = lrelu(bn2, n='act2')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"d_act2\", act2)\n",
    "        #------------\n",
    "\n",
    "        #16x16x256\n",
    "        conv3 = tf.layers.conv2d(act2, c8, kernel_size=[5,5], strides=[2, 2], padding=\"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv3')\n",
    "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn3')\n",
    "        act3 = lrelu(bn3, n='act3')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"d_act3\", act3)\n",
    "        #------------\n",
    "\n",
    "        #8x8x512\n",
    "        conv4 = tf.layers.conv2d(act3, c16, kernel_size=[5,5], strides=[2, 2], padding=\"SAME\",\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv4')\n",
    "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay=0.9, updates_collections=None, scope='bn4')\n",
    "        act4 = lrelu(bn4, n='act4')\n",
    "        #------------\n",
    "        #tf.summary.histogram(\"d_act4\", act4)\n",
    "        #------------\n",
    "\n",
    "        dim = int(np.prod(act4.get_shape()[1:]))\n",
    "        fc1 = tf.reshape(act4, shape=[-1, dim], name='fc1')\n",
    "\n",
    "        w2 = tf.get_variable('w2', shape=[fc1.shape[-1], 1], dtype=tf.float32,\n",
    "                        initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "        b2 = tf.get_variable('b2', shape=[1], dtype=tf.float32, \n",
    "                        initializer = tf.constant_initializer(0.0))\n",
    "        logits = tf.add(tf.matmul(fc1, w2), b2, name='logits')\n",
    "        acted_out = tf.nn.sigmoid(logits)\n",
    "        return logits, acted_out\n",
    "\n",
    "#----plot single-----------\n",
    "def plot_single(samples):\n",
    "  fig = plt.figure(figsize=(8,8))\n",
    "  gs = gridspec.GridSpec(2,2)\n",
    "  gs.update(hspace=0.05, wspace=0.05)\n",
    "\n",
    "  for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.imshow(((sample/2)+0.5), cmap='Greys_r')\n",
    "    \n",
    "    \n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "  fig = plt.figure(figsize=(8,8))\n",
    "  gs = gridspec.GridSpec(8,8)\n",
    "  gs.update(hspace=0.05, wspace=0.05)\n",
    "\n",
    "  for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.imshow(((sample/2)+0.5), cmap='Greys_r')\n",
    "    \n",
    "    \n",
    "  return fig\n",
    "\n",
    "\n",
    "  \n",
    "def train():\n",
    "    random_dim = 100\n",
    "    print((os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "\n",
    "  \n",
    "    with tf.variable_scope('input'):\n",
    "        real_image = tf.placeholder(tf.float32, shape=[None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "        random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='random_input')\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    \n",
    "    fake_image = generator(random_input, random_dim, is_train)\n",
    "    sample_fake = generator(random_input, random_dim, is_train, reuse=True)\n",
    "    real_logits, real_result = discriminator(real_image, is_train)\n",
    "    fake_logits, fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "  \n",
    "    d_loss1 = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=real_logits, labels = tf.ones_like(real_logits)))\n",
    "    d_loss2 = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=fake_logits, labels = tf.zeros_like(fake_logits)))\n",
    "    d_loss = d_loss1 + d_loss2\n",
    "  \n",
    "    g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=fake_logits, labels = tf.ones_like(fake_logits)))\n",
    "  \n",
    "    #------------------\n",
    "    #tf.summary.scalar(\"d_loss1\",d_loss1)\n",
    "    #tf.summary.scalar(\"d_loss2\",d_loss2)\n",
    "    #tf.summary.scalar(\"d_loss\", d_loss)\n",
    "    #tf.summary.scalar(\"g_loss\", g_loss)\n",
    "    #------------------\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [ var for var in t_vars if 'dis' in var.name ]\n",
    "    g_vars = [ var for var in t_vars if 'gen' in var.name ]\n",
    "    \n",
    "  \n",
    "    trainer_d = tf.train.RMSPropOptimizer(learning_rate = 2e-4).minimize(d_loss, var_list=d_vars)\n",
    "    trainer_g = tf.train.RMSPropOptimizer(learning_rate = 2e-4).minimize(g_loss, var_list=g_vars)\n",
    "  \n",
    "\n",
    "    d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in d_vars]\n",
    "  \n",
    "    batch_size = BATCH_SIZE\n",
    "    image_batch, samples_num = process_data()\n",
    "  \n",
    "    batch_num = int(samples_num/batch_size)\n",
    "    total_batch = 0 \n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "  \n",
    "    ckpt = tf.train.latest_checkpoint('model2/'+version)\n",
    "    if ckpt!= None:\n",
    "      saver.restore(sess, ckpt)\n",
    "      \n",
    "    #----------------\n",
    "    #train_writer = tf.summary.FileWriter(\"tensorboard/1/train\", sess.graph)\n",
    "    #----------------\n",
    "        \n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "  \n",
    "    print('total training samples num : %d'% samples_num)\n",
    "    print('batch_size : %d , num Batch : %d, epochs : %d'%(batch_size, batch_num, EPOCH))\n",
    "    print('start training ...............')\n",
    "    \n",
    "\n",
    "    for i in range(EPOCH):\n",
    "    #for i in range(EPOCH):\n",
    "      print(i)\n",
    "      print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "      #s=\"\"\n",
    "      for j in range(batch_num):\n",
    "      \n",
    "        #-------------\n",
    "        #merge = tf.summary.merge_all()\n",
    "        #-------------\n",
    "        \n",
    "        train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "        \n",
    "        train_image = sess.run(image_batch)\n",
    "     \n",
    "        #-----------------clip------\n",
    "        sess.run(d_clip)\n",
    "      \n",
    "        _, dLoss = sess.run([trainer_d, d_loss], feed_dict={random_input:train_noise, real_image:train_image, is_train:True})\n",
    "        _, gLoss = sess.run([trainer_g, g_loss], feed_dict={random_input:train_noise, is_train:True})\n",
    "        #summary = sess.run(merge, feed_dict={random_input:train_noise, real_image:train_image, is_train:True})\n",
    "        #----------------\n",
    "        #summary,_, dLoss,_, gLoss = sess.run([merge,trainer_d, d_loss, trainer_g, g_loss], feed_dict={random_input:train_noise, real_image:train_image, is_train:True})\n",
    "        #------------------------------\n",
    "        #s = summary\n",
    "        #if j==233:\n",
    "        ## train_writer.add_summary(summary, i)\n",
    "        \n",
    "      #train_writer.add_summary(s, i)\n",
    "\n",
    "      if i== 0 or i%30==0 :\n",
    "        print(\"saving model..........\")\n",
    "        if not os.path.exists('model2/'+version):\n",
    "          os.makedirs('model2/'+version)\n",
    "        saver.save(sess,'model2/'+version+'/'+str(i))\n",
    " \n",
    "      if i%30 == 0:\n",
    "        if not os.path.exists(newLand_path):\n",
    "          os.makedirs(newLand_path)\n",
    "          \n",
    "        \n",
    "      sample_noise = np.random.uniform(-1.0, 1.0, size=[64, random_dim]).astype(np.float32)\n",
    "      imgtest = sess.run(fake_image, feed_dict={random_input:sample_noise, is_train:False})\n",
    "      \n",
    "      print('Train:[%d], d_loss:%f, g_loss:%f'%(i, dLoss, gLoss))\n",
    "\n",
    "      \n",
    "      fig = plot(imgtest)\n",
    "      plt.savefig('newLand4/land_{}.jpg'.format(str(i)+\"_\"+\"d_\"+str(dLoss)+\"_g_\"+str(gLoss)), bbox_inches='tight')\n",
    "      plt.close()\n",
    "      \n",
    "      #--------plot single-------------\n",
    "      sample_noise_single = np.random.uniform(-1.0, 1.0, size=[4, random_dim]).astype(np.float32)\n",
    "      imgtest_single = sess.run(fake_image, feed_dict={random_input:sample_noise_single, is_train:False})\n",
    "\n",
    "      \n",
    "      fig = plot_single(imgtest_single)\n",
    "      plt.savefig('newLand4/land2_{}.jpg'.format(str(i)+\"_\"+\"d_\"+str(dLoss)+\"_g_\"+str(gLoss)), bbox_inches='tight')\n",
    "      plt.close()\n",
    "      #----------------------------------\n",
    "\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "# def test():\n",
    "    # random_dim = 100\n",
    "    # with tf.variable_scope('input'):\n",
    "        # real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "        # random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
    "        # is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    # # wgan\n",
    "    # fake_image = generator(random_input, random_dim, is_train)\n",
    "    # real_result = discriminator(real_image, is_train)\n",
    "    # fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "    # sess = tf.InteractiveSession()\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "    # variables_to_restore = slim.get_variables_to_restore(include=['gen'])\n",
    "    # print(variables_to_restore)\n",
    "    # saver = tf.train.Saver(variables_to_restore)\n",
    "    # ckpt = tf.train.latest_checkpoint('./model/' + version)\n",
    "    # saver.restore(sess, ckpt)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "      train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GANART.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
