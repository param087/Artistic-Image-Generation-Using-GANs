{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "dM_8_cpEh-wh",
    "outputId": "dc6da4f5-bfea-4819-ab80-7525726f14a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
      "··········\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "Please enter the verification code: Access token retrieved correctly.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ZRNvghNCiSab",
    "outputId": "8af8cccb-8967-41aa-d697-2816571fab9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libxext6:amd64.\n",
      "(Reading database ... 18106 files and directories currently installed.)\n",
      "Preparing to unpack .../libxext6_2%3a1.3.3-1_amd64.deb ...\n",
      "Unpacking libxext6:amd64 (2:1.3.3-1) ...\n",
      "Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../x11-common_1%3a7.7+19ubuntu3_all.deb ...\n",
      "Unpacking x11-common (1:7.7+19ubuntu3) ...\n",
      "Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../libice6_2%3a1.0.9-2_amd64.deb ...\n",
      "Unpacking libice6:amd64 (2:1.0.9-2) ...\n",
      "Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../libsm6_2%3a1.2.2-1_amd64.deb ...\n",
      "Unpacking libsm6:amd64 (2:1.2.2-1) ...\n",
      "Setting up libxext6:amd64 (2:1.3.3-1) ...\n",
      "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
      "Setting up x11-common (1:7.7+19ubuntu3) ...\n",
      "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libice6:amd64 (2:1.0.9-2) ...\n",
      "Setting up libsm6:amd64 (2:1.2.2-1) ...\n",
      "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
      "/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "\n",
    "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
    "\n",
    "import os\n",
    "os.chdir('drive/dl/GAN/')\n",
    "os.getcwd()\n",
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "3DtalgkzikDI",
    "outputId": "ef53e6c0-5989-4a6e-a8ae-37654ceaf35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "INFO:tensorflow:Restoring parameters from ./model/newART3/90\n",
      "total training sample num:14938\n",
      "batch size: 64, batch num per epoch: 233, epoch num: 5000\n",
      "start training...\n",
      "91\n",
      "2018-03-10 02:22:18\n",
      "train:[91],d_loss:0.505516,g_loss:0.969529\n",
      "92\n",
      "2018-03-10 03:57:06\n",
      "train:[92],d_loss:0.355504,g_loss:2.428835\n",
      "93\n",
      "2018-03-10 04:01:30\n",
      "train:[93],d_loss:1.940821,g_loss:4.809469\n",
      "94\n",
      "2018-03-10 04:05:54\n",
      "train:[94],d_loss:1.038922,g_loss:3.674238\n",
      "95\n",
      "2018-03-10 04:10:17\n",
      "train:[95],d_loss:1.421636,g_loss:1.969218\n",
      "96\n",
      "2018-03-10 04:14:38\n",
      "train:[96],d_loss:1.403251,g_loss:6.355890\n",
      "97\n",
      "2018-03-10 04:19:01\n",
      "train:[97],d_loss:0.444719,g_loss:4.675386\n",
      "98\n",
      "2018-03-10 04:23:23\n",
      "train:[98],d_loss:0.395163,g_loss:2.160926\n",
      "99\n",
      "2018-03-10 04:27:44\n",
      "train:[99],d_loss:0.523266,g_loss:4.433130\n",
      "100\n",
      "2018-03-10 04:32:05\n",
      "train:[100],d_loss:1.762568,g_loss:7.830241\n",
      "101\n",
      "2018-03-10 04:36:26\n",
      "train:[101],d_loss:0.577389,g_loss:4.299891\n",
      "102\n",
      "2018-03-10 04:40:47\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import scipy.misc\n",
    "from utils import *\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "HEIGHT, WIDTH, CHANNEL = 128, 128, 3\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 5000\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "version = 'newART3'\n",
    "newPoke_path = './' + version\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "  return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def lrelu(x, n, leak=0.2): \n",
    "  return tf.maximum(x, leak * x, name=n) \n",
    " \n",
    "def process_data():   \n",
    "    current_dir = os.getcwd()\n",
    "    # parent = os.path.dirname(current_dir)\n",
    "    pokemon_dir = os.path.join(current_dir, 'train_landscape_128')\n",
    "    images = []\n",
    "    for each in os.listdir(pokemon_dir):\n",
    "        images.append(os.path.join(pokemon_dir,each))\n",
    "    # print images    \n",
    "    all_images = tf.convert_to_tensor(images, dtype = tf.string)\n",
    "    \n",
    "    images_queue = tf.train.slice_input_producer(\n",
    "                                        [all_images])\n",
    "                                        \n",
    "    content = tf.read_file(images_queue[0])\n",
    "    image = tf.image.decode_jpeg(content, channels = CHANNEL)\n",
    "    # sess1 = tf.Session()\n",
    "    # print sess1.run(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n",
    "    # noise = tf.Variable(tf.truncated_normal(shape = [HEIGHT,WIDTH,CHANNEL], dtype = tf.float32, stddev = 1e-3, name = 'noise')) \n",
    "    # print image.get_shape()\n",
    "    size = [HEIGHT, WIDTH]\n",
    "    image = tf.image.resize_images(image, size)\n",
    "    image.set_shape([HEIGHT,WIDTH,CHANNEL])\n",
    "    # image = image + noise\n",
    "    # image = tf.transpose(image, perm=[2, 0, 1])\n",
    "    # print image.get_shape()\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    iamges_batch = tf.train.shuffle_batch(\n",
    "                                    [image], batch_size = BATCH_SIZE,\n",
    "                                    num_threads = 4, capacity = 2000 + 3* BATCH_SIZE,\n",
    "                                    min_after_dequeue = 2000)\n",
    "    num_images = len(images)\n",
    "\n",
    "    return iamges_batch, num_images\n",
    "\n",
    "def generator(input, random_dim, is_train, reuse=False):\n",
    "    c4, c8, c16, c32, c64 = 512, 256, 128, 64, 32 # channel num\n",
    "    s4 = 4\n",
    "    output_dim = CHANNEL  # RGB image\n",
    "    with tf.variable_scope('gen') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        w1 = tf.get_variable('w1', shape=[random_dim, s4 * s4 * c4], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable('b1', shape=[c4 * s4 * s4], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "        flat_conv1 = tf.add(tf.matmul(input, w1), b1, name='flat_conv1')\n",
    "        # 4*4*512\n",
    "        conv1 = tf.reshape(flat_conv1, shape=[-1, s4, s4, c4], name='conv1')\n",
    "        bn1 = tf.contrib.layers.batch_norm(conv1, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn1')\n",
    "        act1 = tf.nn.relu(bn1, name='act1')\n",
    "        # 8*8*256\n",
    "        conv2 = tf.layers.conv2d_transpose(act1, c8, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv2')\n",
    "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n",
    "        act2 = tf.nn.relu(bn2, name='act2')\n",
    "        # 16*16*128\n",
    "        conv3 = tf.layers.conv2d_transpose(act2, c16, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv3')\n",
    "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n",
    "        act3 = tf.nn.relu(bn3, name='act3')\n",
    "        # 32*32*64\n",
    "        conv4 = tf.layers.conv2d_transpose(act3, c32, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv4')\n",
    "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n",
    "        act4 = tf.nn.relu(bn4, name='act4')\n",
    "        # 64*64*32\n",
    "        conv5 = tf.layers.conv2d_transpose(act4, c64, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv5')\n",
    "        bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn5')\n",
    "        act5 = tf.nn.relu(bn5, name='act5')\n",
    "        \n",
    "        #128*128*3\n",
    "        conv6 = tf.layers.conv2d_transpose(act5, output_dim, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name='conv6')\n",
    "        # bn6 = tf.contrib.layers.batch_norm(conv6, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn6')\n",
    "        act6 = tf.nn.tanh(conv6, name='act6')\n",
    "        return act6\n",
    "\n",
    "\n",
    "def discriminator(input, is_train, reuse=False):\n",
    "    c2, c4, c8, c16 = 64, 128, 256, 512  # channel num: 64, 128, 256, 512\n",
    "    with tf.variable_scope('dis') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        # 64*64*64\n",
    "        conv1 = tf.layers.conv2d(input, c2, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv1')\n",
    "        # bn1 = tf.contrib.layers.batch_norm(conv1, is_training = is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope = 'bn1')\n",
    "        act1 = lrelu(conv1, n='act1')\n",
    "        # 32*32*128\n",
    "        conv2 = tf.layers.conv2d(act1, c4, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv2')\n",
    "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n",
    "        act2 = lrelu(bn2, n='act2')\n",
    "        # 16*16*256\n",
    "        conv3 = tf.layers.conv2d(act2, c8, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv3')\n",
    "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n",
    "        act3 = lrelu(bn3, n='act3')\n",
    "        # 8*8*512\n",
    "        conv4 = tf.layers.conv2d(act3, c16, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name='conv4')\n",
    "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n",
    "        act4 = lrelu(bn4, n='act4')\n",
    "        # # 8*8*256\n",
    "        # conv5 = tf.layers.conv2d(act4, c32, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
    "                                 # kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 # name='conv5')\n",
    "        # bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn5')\n",
    "        # act5 = lrelu(bn5, n='act5')\n",
    "        \n",
    "        # start from act4\n",
    "        dim = int(np.prod(act4.get_shape()[1:]))\n",
    "        fc1 = tf.reshape(act4, shape=[-1, dim], name='fc1')\n",
    "        # w1 = tf.get_variable('w1', shape=[fc1.shape[-1], 512], dtype=tf.float32,\n",
    "                             # initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # b1 = tf.get_variable('b1', shape=[512], dtype=tf.float32,\n",
    "                             # initializer=tf.constant_initializer(0.0))\n",
    "        # bnf = tf.contrib.layers.batch_norm(tf.matmul(fc1,w1), is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bnf')\n",
    "        # act_fc1 = lrelu(tf.nn.bias_add(bnf, b1),n = 'actf')\n",
    "        \n",
    "        w2 = tf.get_variable('w2', shape=[fc1.shape[-1], 1], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable('b2', shape=[1], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # wgan just get rid of the sigmoid\n",
    "        logits = tf.add(tf.matmul(fc1, w2), b2, name='logits')\n",
    "        # dcgan\n",
    "        acted_out = tf.nn.sigmoid(logits)\n",
    "        return logits , acted_out\n",
    "def plot(samples):\n",
    "  \n",
    "  fig = plt.figure(figsize=(8,8))\n",
    "  gs = gridspec.GridSpec(8,8)\n",
    "  gs.update(hspace=0.05, wspace=0.05)\n",
    "  \n",
    "  for i, sample in enumerate(samples):\n",
    "    \n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.imshow(((sample/2)+0.5), cmap='Greys_r')\n",
    "       \n",
    "  return fig\n",
    "\n",
    "def train():\n",
    "    \n",
    "    random_dim = 100\n",
    "    print( (os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "    \n",
    "    with tf.variable_scope('input'):\n",
    "        real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "        random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    # wgan\n",
    "    #fake_image = generator(random_input, random_dim, is_train)\n",
    "    #real_result = discriminator(real_image, is_train)\n",
    "    #fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "    \n",
    "    #d_loss = tf.reduce_mean(fake_result) - tf.reduce_mean(real_result)  # This optimizes the discriminator.\n",
    "    #g_loss = -tf.reduce_mean(fake_result)  # This optimizes the generator.\n",
    "    \n",
    "    # # dcgan loss\n",
    "    fake_image = generator(random_input, random_dim, is_train)\n",
    "    sample_fake = generator(random_input, random_dim, is_train, reuse = True)\n",
    "    real_logits, real_result = discriminator(real_image, is_train)\n",
    "    fake_logits, fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "    \n",
    "    d_loss1 = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = real_logits, labels = tf.ones_like(real_logits)))\n",
    "    d_loss2 = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = fake_logits, labels = tf.zeros_like(fake_logits)))\n",
    "    \n",
    "    d_loss = d_loss1 + d_loss2\n",
    "    \n",
    "    g_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = fake_logits, labels = tf.ones_like(fake_logits)))\n",
    "            \n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if 'dis' in var.name]\n",
    "    g_vars = [var for var in t_vars if 'gen' in var.name]\n",
    "    # test\n",
    "    # print(d_vars)\n",
    "    trainer_d = tf.train.RMSPropOptimizer(learning_rate=2e-4).minimize(d_loss, var_list=d_vars)\n",
    "    trainer_g = tf.train.RMSPropOptimizer(learning_rate=2e-4).minimize(g_loss, var_list=g_vars)\n",
    "    # clip discriminator weights\n",
    "    d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in d_vars]\n",
    "\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    image_batch, samples_num = process_data()\n",
    "    \n",
    "    batch_num = int(samples_num / batch_size)\n",
    "    total_batch = 0\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    # continue training\n",
    "    ckpt = tf.train.latest_checkpoint('./model/' + version)\n",
    "    if ckpt!=None:\n",
    "      \n",
    "      saver.restore(sess, ckpt)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    print ('total training sample num:%d' % samples_num)\n",
    "    print ('batch size: %d, batch num per epoch: %d, epoch num: %d' % (batch_size, batch_num, EPOCH))\n",
    "    print ('start training...')\n",
    "    for i in np.arange(91,EPOCH):\n",
    "      print(i)\n",
    "      print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "      for j in range(batch_num):\n",
    "        #d_iters = 5\n",
    "        #g_iters = 1\n",
    "\n",
    "        train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "        #for k in range(d_iters):\n",
    "          \n",
    "        train_image = sess.run(image_batch)\n",
    "          #wgan clip weights\n",
    "        sess.run(d_clip)\n",
    "                \n",
    "                # Update the discriminator\n",
    "        _, dLoss = sess.run([trainer_d, d_loss],\n",
    "                                    feed_dict={random_input: train_noise, real_image: train_image, is_train: True})\n",
    "\n",
    "            # Update the generator\n",
    "        #for k in range(g_iters):\n",
    "          \n",
    "          # train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "        _, gLoss = sess.run([trainer_g, g_loss],\n",
    "                                    feed_dict={random_input: train_noise, is_train: True})\n",
    "\n",
    "          # print 'train:[%d/%d],d_loss:%f,g_loss:%f' % (i, j, dLoss, gLoss)\n",
    "            \n",
    "      # save check point every 500 epoch\n",
    "      if i%30 == 0:\n",
    "        \n",
    "        if not os.path.exists('./model/' + version):\n",
    "          os.makedirs('./model/' + version)\n",
    "        print(\"saving model........\")\n",
    "        saver.save(sess, './model/' +version + '/' + str(i))  \n",
    "      if i%10 == 0:\n",
    "        # save images\n",
    "        if not os.path.exists(newPoke_path):\n",
    "          os.makedirs(newPoke_path)\n",
    "          sample_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "          imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise, is_train: False})\n",
    "          # imgtest = imgtest * 255.0\n",
    "          # imgtest.astype(np.uint8)\n",
    "          save_images(imgtest, [8,8] ,newPoke_path + '/epoch' + str(i) + '.jpg')\n",
    "          \n",
    "        #print ('train:[%d],d_loss:%f,g_loss:%f' % (i, dLoss, gLoss))\n",
    "      \n",
    "      \n",
    "      sample_noise = np.random.uniform(-1.0, 1.0, size=[64, random_dim]).astype(np.float32)\n",
    "      imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise,  is_train: False})\n",
    "      print ('train:[%d],d_loss:%f,g_loss:%f' % (i, dLoss, gLoss))\n",
    "            \n",
    "      fig = plot(imgtest)\n",
    "      plt.savefig('newArt3/land{}.jpg'.format(str(i)), bbox_inches='tight')\n",
    "      plt.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "\n",
    "# def test():\n",
    "    # random_dim = 100\n",
    "    # with tf.variable_scope('input'):\n",
    "        # real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "        # random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
    "        # is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    # # wgan\n",
    "    # fake_image = generator(random_input, random_dim, is_train)\n",
    "    # real_result = discriminator(real_image, is_train)\n",
    "    # fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "    # sess = tf.InteractiveSession()\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "    # variables_to_restore = slim.get_variables_to_restore(include=['gen'])\n",
    "    # print(variables_to_restore)\n",
    "    # saver = tf.train.Saver(variables_to_restore)\n",
    "    # ckpt = tf.train.latest_checkpoint('./model/' + version)\n",
    "    # saver.restore(sess, ckpt)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    # test()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "art.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
